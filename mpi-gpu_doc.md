# Hybrid GPU-programming: Combining MPI with GPU-directive models

# Summary 

Parallel computing involving communication between heterogenous systems, especially CPU (central processing unit) and GPU (graphics processing unit), permits to improve the performance of the computation on modern HPC (high-performance computing) systems by many orders of magnitude (see …). This in turn allows us to address large scientific computational problems, which would not be possible using conventional CPU-based approaches. Here, although the GPU-directive programming models OpenACC and OpenMP asynchronous offer the potential to carry out computations across multiple GPUs, the partition of the computation is limited to a single GPU node. Extending the computation to explore multiple GPU nodes requires combining MPI (message passing interface) with either OpenACC or OpenMP application programming interfaces (APIs). Note that CUDA is another alternative. This is not addressed in this tutorial; it will however be the subject of an ulterior tutorial. 

Combining MPI with OpenACC or OpenMP offloading APIs offers the potential to fully utilizing the capacity of multiple GPUs within multiple GPU partitions in modern clusters and supercomputers, thus rendering the HPC applications efficient. This tutorial is thus motivated by the need of guiding users, who are familiar with MPI, in porting their MPI-codes to GPU systems and towards exploring exascale platforms, such as the supercomputer LUMI. 

As we will show, this multi-GPU approach has the potential of reducing the computing time caused by transferring data via the host-memory during heterogenous communications. Here instead the MPI-library can directly access the GPU-device memory, thus rending the performance effective (efficient).

By the end of this tutorial, we expect the users to learn about

- How to implement a mixture of the low-level MPI with the OpenACC or OpenMP APIs.    
- MPI implementation with the GPU-direct/indirect memory access.


This tutorial is organized as follows: In section I, we describe the implementation of the low-level MPI alone using an application based on solving the Laplace equation. In section II, we extend the MPI-application to incorporate a GPU approach. This is done by combining MPI with OpenACC/OpenMP AIPs. Here we will address both accelerator and non-accelerator -aware MPI-library (i.e. MPI with direct memory access vs MPI with indirect memory access). Section III is devoted to the performance testing. Section II concludes the tutorial.

```{contents} Table of Contents
```

#### Table of Contents

- [Summary](#summary)
- [Implementation of MPI](#implementation-of-mpi)
- [Implementation of MPI-OpenACC and -OpenMP offloading](#implementation-of-mpi-openacc-and--openmp-offloading)
- [Performance analysis](#performance-analysis)
- [Conclusion](#conclusion)

(implementation-mpi)=
# Implementation of MPI  

The MPI programming model is widely used in the scientific community for intensive parallel computing that requires distributed memory among multiple nodes. In this section, we implement the low-level [MPI standard]( https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf) approach to parallelise our `Fortran` application, which is based on solving the Laplace equation in a uniform 2D-grid. Details about the numerical method can be found here. 

A starting point here is to generate initial conditions and distribute them among different processes in a communicator group. This is done using the `MPI_Scatter` operation, which distributes the generated data from the processes 0 (root) to processes labeled *myid* (i.e. the rank of an MPI process) defined in the range [*myid=0*, *myid=nproc-1*], where *nproc* is the total number of processes. To be specific, the data, which are initially of dimension *$n_{x}$* x *$n_{y}$* (cf. Fig. 1(a)) are subdivided along the y-direction into sub-arrays of dimension *$n_{x}$* x *$n_{yp}$*, where *$n_{yp}$=$n_{y}$/nproc*, such that each sub-array can be used by each MPI process as shown in Fig. 1(b). 

The initial conditions (ICs) we consider are random and are generated by the routine ` RANDOM_NUMBER`. Note that MPI-programs require incorporating the mpi module (i.e. `use mpi`) or including the header file mpif.h (i.e. `include ‘mpif.h’`). The meaning of each called MPI function is included briefly as a comment in the source code.  

A subsequent step is to iterate the ICs using an appropriate algorithm. We chose Lanczos algorithm as described in our previous [tutorial](https://documentation.sigma2.no/code_development/guides/converting_acc2omp/openacc2openmp.html) (see also [here](https://arxiv.org/abs/2201.11811)). In the iterative scheme, the distributed data along the y-direction needs to be updated; this is because the data at the boundaries of each sub-array in each MPI process are initially set to zero. For instance, computing the new array *f_k(:,1)* requires updating the elements *f(:,0)* on each process; similarly for *f(:,nyp+1)* (see the loop region defined by lines). A key element here is to transfer the data in the boundaries between the neighboring MPI processes at each iteration. This is schematically illustrated in Fig. 2, which in turn is transformed into a few MPI lines using a blocking communication mode characterized by the MPI functions `MPI_Send()` and `MPI_Recv()` (see lines ….). The blocking mode here means that the **send** and **receive** operations do not return until the message data is available to be re-used. In other words, the operations are completed once the message is buffered.  Note that there are three additional blocking modes for the **send** operation. These modes, however, are not addressed in the present documentation. We thus refer to the [MPI documentation]( https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf) for further description. 

Updating the data in the boundaries is a key challenge in this example. This ensures the correctness of the computed maximum between the new and the old arrays at each iteration, thus permitting to check the convergence of the results. The computed maximum is done using the  `MPI_Allreduce` operation, in which the result is returned to all MPI processes of the specified communicator group. 

To check the correctness of the results, one can compute the sum of all the elements or eventually display the converged data either in 1D or 2D for comparison. For this reason, we introduce the `MPI_Gather` operation, which allows aggregating the data from each MPI process and make them available only in the root process. This option, however, might become time consuming and eventually might lead to segmentation error when increasing the size of the data.


(implementation-mpi-acc-omp)=
# Implementation of MPI-OpenACC and -OpenMP offloading
## GPU-non-aware MPI
### The hybrid MPI-OpenACC

### The hybrid MPI-OpenMP offloading

## GPU-aware MPI
### The hybrid MPI-OpenACC

### The hybrid MPI-OpenMP offloading

(performance-testing)=
# Performance analysis

(conclusion)=
# Conclusion
